{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code is for my NLP Udemy class, which can be found at:\n",
    "# https://deeplearningcourses.com/c/data-science-natural-language-processing-in-python\n",
    "# https://www.udemy.com/data-science-natural-language-processing-in-python\n",
    "# It is written in such a way that tells a story.\n",
    "# i.e. So you can follow a thought process of starting from a\n",
    "# simple idea, hitting an obstacle, overcoming it, etc.\n",
    "# i.e. It is not optimized for anything.\n",
    "\n",
    "# Author: http://lazyprogrammer.me\n",
    "% matplotlib notebook\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n",
    "\n",
    "# load the reviews\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('electronics/positive.review').read())\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')\n",
    "\n",
    "# there are more positive reviews than negative reviews\n",
    "# so let's take a random sample so we have balanced classes\n",
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]\n",
    "\n",
    "# first let's just try to tokenize the text using nltk's tokenizer\n",
    "# let's take the first review for example:\n",
    "# t = positive_reviews[0]\n",
    "# nltk.tokenize.word_tokenize(t.text)\n",
    "#\n",
    "# notice how it doesn't downcase, so It != it\n",
    "# not only that, but do we really want to include the word \"it\" anyway?\n",
    "# you can imagine it wouldn't be any more common in a positive review than a negative review\n",
    "# so it might only add noise to our model.\n",
    "# so let's create a function that does all this pre-processing for us\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# create a word-to-index map so that we can create our word-frequency vectors later\n",
    "# let's also save the tokenized versions so we don't have to tokenize again later\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "\n",
    "# now let's create our input matrices\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # normalize it before setting label\n",
    "    x[-1] = label\n",
    "    return x\n",
    "\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "# (N x D+1 matrix - keeping them together for now so we can shuffle more easily later\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "# shuffle the data and create train/test splits\n",
    "# try it multiple times!\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "# last 100 rows will be test\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print \"Classification rate:\", model.score(Xtest, Ytest)\n",
    "\n",
    "\n",
    "# let's look at the weights for each word\n",
    "# try it with different threshold values!\n",
    "threshold = 0.5\n",
    "for word, index in word_index_map.iteritems():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print word, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "import pdb\n",
    "import sklearn\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from bs4 import BeautifulSoup\n",
    "positive_reviews1 = BeautifulSoup(open('electronics/positive.review').read())\n",
    "positive_reviews = positive_reviews1.findAll('review_text')\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<haha> nanana</haha>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = BeautifulSoup('<haha> nanana</haha>')\n",
    "b.findAll('haha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# see data\n",
    "if False: print positive_reviews1\n",
    "if False: print positive_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'among', 'an', 'and', 'another', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'are', 'area', 'areas', 'around', 'as', 'ask', 'asked', 'asking', 'asks', 'at', 'away', 'b', 'back', 'backed', 'backing', 'backs', 'be', 'became', 'because', 'become', 'becomes', 'been', 'before', 'began', 'behind', 'being', 'beings', 'best', 'better', 'between', 'big', 'both', 'but', 'by', 'c', 'came', 'can', 'cannot', 'case', 'cases', 'certain', 'certainly', 'clear', 'clearly', 'come', 'could', 'd', 'did', 'differ', 'different', 'differently', 'do', 'does', 'done', 'down', 'down', 'downed', 'downing', 'downs', 'during', 'e', 'each', 'early', 'either', 'end', 'ended', 'ending', 'ends', 'enough', 'even', 'evenly', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'f', 'face', 'faces', 'fact', 'facts', 'far', 'felt', 'few', 'find', 'finds', 'first', 'for', 'four', 'from', 'full', 'fully', 'further', 'furthered', 'furthering', 'furthers', 'g', 'gave', 'general', 'generally', 'get', 'gets', 'give', 'given', 'gives', 'go', 'going', 'good', 'goods', 'got', 'great', 'greater', 'greatest', 'group', 'grouped', 'grouping', 'groups', 'h', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'herself', 'high', 'high', 'high', 'higher', 'highest', 'him', 'himself', 'his', 'how', 'howevhowevhoif', 'important', 'in', 'interest', 'interested', 'interesting', 'interests', 'into', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kind', 'knew', 'know', 'known', 'knows', 'l', 'large', 'largely', 'last', 'later', 'latest', 'least', 'less', 'let', 'lets', 'like', 'likely', 'long', 'longer', 'longest', 'm', 'made', 'make', 'making', 'man', 'many', 'may', 'me', 'member', 'members', 'men', 'might', 'more', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', 'n', 'nnnnnsary', 'need', 'needed', 'needing', 'needs', 'never', 'new', 'new', 'newer', 'newest', 'next', 'no', 'nobody', 'non', 'noone', 'not', 'nothing', 'now', 'nowhere', 'number', 'numbers', 'o', 'of', 'off', 'often', 'old', 'older', 'oldest', 'on', 'once', 'one', 'only', 'open', 'opened', 'opening', 'opens', 'or', 'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'our', 'out', 'over', 'p', 'part', 'parted', 'parting', 'parts', 'per', 'perhaps', 'place', 'places', 'point', 'pointed', 'pointing', 'points', 'possible', 'present', 'presented', 'presenting', 'presents', 'problem', 'problems', 'put', 'puts', 'q', 'quite', 'r', 'rather', 'really', 'right', 'right', 'room', 'rooms', 's', 'said', 'same', 'saw', 'say', 'says', 'second', 'seconds', 'see', 'seem', 'seemed', 'seeming', 'seems', 'sees', 'several', 'shall', 'she', 'should', 'show', 'showed', 'showing', 'shows', 'side', 'sides', 'since', 'small', 'smaller', 'smallest', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', 'state', 'states', 'still', 'still', 'such', 'sure', 't', 'take', 'taken', 'than', 'that', 'the', 'their', 'them', 'hen', 'there', 'therefore', 'these', 'they', 'thing', 'things', 'think', 'thinks', 'think', 'those', 'though', 'thought', 'thoughts', 'three', 'through', 'thus', 'to', 'today', 'together', 'too', 'took', 'toward', 'turn', 'turned', 'turning', 'turns', 'two', 'u', 'under', 'until', 'up', 'upon', 'us', 'use', 'used', 'uses', 'v', 'very', 'w', 'want', 'wanted', 'wanting', 'wants', 'was', 'way', 'ways', 'we', 'well', 'wells', 'went', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with', 'within', 'without', 'work', 'worked', 'working', 'works', 'would', 'x', 'y', 'year', 'years', 'yet', 'yyyyyyyung', 'younger', 'youngest', 'your', 'yours', 'z']\n"
     ]
    }
   ],
   "source": [
    "# get a list of stopwords\n",
    "stopwords1  = open('stopwords.txt','r')\n",
    "stopwords = [w.rstrip() for w in stopwords1]\n",
    "# lens = [abs(len(w1)-len(w2)) for w1,w2 in zip(stopwords1,stopwords)]\n",
    "# where_unequal= np.where(lens)\n",
    "# print where_unequal\n",
    "# print [w for ix,w in enumerate(stopwords) if ix in where_unequal]\n",
    "print stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the tokenizer is built over the lemmatizer, and does a sequence of operations\n",
    "import pdb\n",
    "def tokenizer(s): #s is text\n",
    "    \n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [t for t in tokens if len(t)>2]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # does not convert going to go\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "#     print tokens\n",
    "#     pdb.set_trace()\n",
    "    return tokens\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_pos_tokens = []\n",
    "vocab = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    rtokens = tokenizer(review.text)\n",
    "    all_pos_tokens.append(rtokens)\n",
    "    vocab.extend(rtokens)\n",
    "#     pdb.set_trace()\n",
    "vocab = list(set(vocab))\n",
    "all_neg_tokens = []\n",
    "for review in negative_reviews:\n",
    "    rtokens = tokenizer(review.text)\n",
    "    all_neg_tokens.append(rtokens)\n",
    "    vocab.extend(rtokens)\n",
    "vocab = list(set(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'con', u'tip', u'extremely', u'easy', u'carpet', u'you', u'lot', u'cd', u'stacked', u'top', u'poorly', u'designed', u'vertical', u'rack', u'doesnt', u'individual', u'slot', u'cd', u'you', u'bottom', u'stack', u'you', u'basically', u'pull', u'stack', u'putting', u'wa', u'pain', u'bought', u'break', u'piece', u'metal', u'fit', u'guide', u'hole', u'again..poorly', u'designed', u'...', u'doesnt', u'fit', u'cd', u'gap', u'ca', u'loose', u'fitting', u'pro', u'...', u'...', u'...', u'guess', u'hold', u'lot', u'cd', u'...'], [u'nice', u'look', u'tip', u'easily', u'steady', u'rug', u'surface', u'dispite', u'picture', u'box', u'advice', u'you', u'rack', u'hold', u'lot', u'Save', u'money', u'invest', u'nicer', u'sturdy'], [u'bought', u'returned', u'unit', u'Each', u'ha', u'defective', u'finally', u'returning', u'system', u'The', u'DVD', u'player', u'constantly', u'Bad', u'Disc', u'error', u'skip', u'slightest', u'smudge', u'disc', u'The', u'sound', u'quality', u'nice', u'price', u'player', u'doe', u\"n't\", u'essentially', u'useless', u'This', u'complete', u'rip-off', u'price'], [u'inexpensive', u\"n't\", u'breaking', u'pieces..'], [u'pack', u'doing', u'DVD', u'backup', u'failed', u'wa', u'software', u'software', u'Guess', u'disc', u'package', u'bought', u'failed', u'burner', u'reviewer', u'suggested', u'implying', u'people', u'learn', u'read', u'brand', u'doe', u\"n't\", u'require', u'firmware', u'upgrade', u'mention', u'They', u'suck', u'failure', u'rate', u'consensus', u'TDK', u'Sony', u'buying', u'save', u'frustration'], [u'bought', u'disc', u'CompUSA', u'online', u'Well', u'wated', u'buck', u'disc', u'burned', u'Not', u'bang', u'buck', u'burned', u'disc', u'Memorex', u'usually', u'Verbatim', u'Ridata', u'MAM', u'Very', u'disgruntled', u'disc', u'coaster', u'Maybe', u'burner', u\"n't\", u'whatever', u\"n't\", u'buy', u'Memorex', u'product'], [u'The', u'DVDs', u'burned', u'successfully', u'movie', u'excellent', u'quality', u'The', u'burn', u'bad', u'using', u'Plextor', u'DVD', u'burner', u'love', u'try', u'luck', u'DVD', u'brand', u'The', u'failure', u'rate', u'Memorex', u'unacceptable', u'le', u'cost', u'bother', u'time', u'wasted', u'burning', u'coaster'], [u'hope', u'live', u'memorex', u'crap', u'opinion', u'using', u'this', u'product', u'LiteOn', u'SOHW-802S', u'hate', u'Memorex', u'rarely', u'decent', u'burn', u'More', u\"'ll\", u'discover', u'watching', u'movie', u'horrid', u'quality', u'movie', u'skip', u'stutter', u'stop', u'disappointing', u'Unfortunately', u'found', u'late', u'Memorex', u'CMC', u'medium', u'ha', u'0-50', u'success', u'rate', u'...'], [u'While', u'read', u'review', u'this', u'machine', u'Lexar', u'ha', u'serious', u'quality-control', u'For', u'people', u'wa', u'miserable', u'failure', u'latter', u'Sigh', u'Mine', u'wa', u'dead', u'arrival', u'tried', u'device', u'*BOTH*', u'Windows-XP', u'machine', u'wa', u'unrecognized', u'Lexar', u'tech', u'support', u'subsequently', u'proved', u'contradiction', u'term', u'return', u'email', u'request', u'help'], [u'cash', u'offer', u'sounded', u'true', u'...', u'rejected', u'upcs', u'kept', u'copy', u'hounding', u'contacting', u'business', u'bureau', u\"'whoops\", u'mistake', u'fleeting', u'suspicion', u'this', u'wa', u'intentional']]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

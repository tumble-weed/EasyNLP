{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Latent Semantic Analysis\\n\\nThis takes in book titles. the purpose of LSA here is to represent each term (word) \\nin the corpus (the total collection of book titles), as a projection onto documents. \\nThe intuition here is that a word appears in books of many type, for example sex would\\nappear in medical books, romantic books, and anatomy books. So the 'meaning' of sex\\nis then represented by a linear combination of all these different books.\\n\\nAlso to note here is that unlike the sentiment analysis program where the features were\\nfrequencies of the terms in the review (whose sum is normalized to 1.), here the features are\\nindicators, i.e. 'does this word appear in the book title or not?'\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Latent Semantic Analysis\n",
    "\n",
    "This takes in book titles. the purpose of LSA here is to represent each term (word) \n",
    "in the corpus (the total collection of book titles), as a projection onto documents. \n",
    "The intuition here is that a word appears in books of many type, for example sex would\n",
    "appear in medical books, romantic books, and anatomy books. So the 'meaning' of sex\n",
    "is then represented by a linear combination of all these different books.\n",
    "\n",
    "Also to note here is that unlike the sentiment analysis program where the features were\n",
    "frequencies of the terms in the review (whose sum is normalized to 1.), here the features are\n",
    "indicators, i.e. 'does this word appear in the book title or not?'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use this to make matplotlib draw within the notebook, and not launch a new window\n",
    "%matplotlib notebook \n",
    "import nltk\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn # for SVD\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Order of Operations\n",
    "1. Read the Data,Stopwords\n",
    "2. Preprocessing the Data:1. lowercase, Tokenize, lemmatize,  remove stop words\n",
    "                          2. make vocabulary\n",
    "                          3. convert to numeric data\n",
    "\n",
    "3. Fit using SVD\n",
    "4. Visualize the results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['all', 'pointing', 'four', 'go', 'oldest', 'seemed', 'whose', 'certainly', 'presents', 'to', 'asking', 'those', 'under', 'far', 'every', 'presented', 'did', 'turns', 'large', 'p', 'small', 'parted', 'smaller', 'says', 'second', 'further', 'nnnnnsary', 'even', 'what', 'anywhere', 'above', 'new', 'ever', 'full', 'men', 'here', 'youngest', 'let', 'groups', 'others', 'alone', 'along', 'great', 'k', 'put', 'everybody', 'use', 'from', 'working', 'yyyyyyyung', 'two', 'next', 'almost', 'therefore', 'taken', 'until', 'today', 'more', 'knows', 'clearly', 'becomes', 'it', 'downing', 'everywhere', 'known', 'cases', 'must', 'me', 'states', 'room', 'f', 'work', 'itself', 'can', 'mr', 'making', 'my', 'numbers', 'give', 'high', 'something', 'want', 'needs', 'end', 'turn', 'rather', 'how', 'y', 'may', 'after', 'such', 'man', 'a', 'q', 'so', 'keeps', 'order', 'furthering', 'over', 'years', 'ended', 'through', 'still', 'its', 'before', 'group', 'somewhere', 'interesting', 'better', 'differently', 'might', 'non', 'good', 'somebody', 'greater', 'downs', 'they', 'not', 'now', 'gets', 'always', 'l', 'each', 'went', 'side', 'everyone', 'year', 'our', 'out', 'opened', 'since', 'got', 'shows', 'turning', 'differ', 'quite', 'members', 'ask', 'wanted', 'g', 'could', 'needing', 'keep', 'thing', 'place', 'w', 'think', 'first', 'already', 'seeming', 'number', 'one', 'done', 'another', 'open', 'given', 'needed', 'ordering', 'least', 'anyone', 'their', 'too', 'gives', 'interests', 'mostly', 'behind', 'nobody', 'took', 'part', 'herself', 'than', 'kind', 'b', 'showed', 'older', 'likely', 'r', 'were', 'toward', 'and', 'sees', 'turned', 'few', 'say', 'have', 'need', 'seem', 'saw', 'orders', 'that', 'also', 'take', 'which', 'wanting', 'sure', 'shall', 'knew', 'wells', 'most', 'nothing', 'why', 'parting', 'noone', 'later', 'm', 'mrs', 'points', 'fact', 'show', 'ending', 'find', 'state', 'should', 'only', 'going', 'pointed', 'do', 'his', 'get', 'cannot', 'longest', 'during', 'him', 'areas', 'h', 'she', 'x', 'where', 'we', 'see', 'are', 'best', 'said', 'ways', 'away', 'enough', 'smallest', 'between', 'across', 'ends', 'never', 'opening', 'come', 'both', 'c', 'last', 'many', 'against', 's', 'became', 'faces', 'whole', 'asked', 'among', 'point', 'seems', 'furthered', 'furthers', 'puts', 'three', 'been', 'much', 'interest', 'wants', 'worked', 'an', 'present', 'case', 'myself', 'these', 'n', 'will', 'while', 'would', 'backing', 'is', 'thus', 'them', 'someone', 'in', 'different', 'perhaps', 'things', 'make', 'same', 'any', 'member', 'parts', 'several', 'higher', 'used', 'upon', 'uses', 'thoughts', 'off', 'largely', 'well', 'anybody', 'finds', 'thought', 'without', 'greatest', 'very', 'the', 'yours', 'latest', 'newest', 'just', 'less', 'being', 'when', 'rooms', 'facts', 'yet', 'had', 'lets', 'interested', 'has', 'gave', 'around', 'big', 'showing', 'possible', 'early', 'know', 'like', 'd', 't', 'fully', 'become', 'works', 'grouping', 'because', 'old', 'often', 'some', 'back', 'thinks', 'for', 'though', 'per', 'everything', 'does', 'either', 'be', 'who', 'seconds', 'nowhere', 'although', 'by', 'on', 'about', 'goods', 'asks', 'anything', 'of', 'o', 'or', 'into', 'within', 'down', 'beings', 'right', 'your', 'her', 'area', 'downed', 'there', 'long', 'way', 'hen', 'was', 'opens', 'himself', 'but', 'newer', 'highest', 'with', 'he', 'made', 'places', 'whether', 'j', 'up', 'us', 'problem', 'z', 'clear', 'v', 'ordered', 'certain', 'general', 'as', 'at', 'face', 'again', 'no', 'generally', 'backs', 'grouped', 'other', 'really', 'howevhowevhoif', 'felt', 'problems', 'important', 'sides', 'began', 'younger', 'e', 'longer', 'came', 'backed', 'together', 'u', 'presenting', 'evenly', 'having', 'once'])\n",
      "['all', 'pointing', 'show', 'felt', 'ending', 'four', 'area', 'go', 'oldest', 'find', 'seemed', 'whose', 'certainly', 'with', 'state', 'should', 'presents', 'to', 'only', 'going', 'present', 'under', 'pointed', 'do', 'his', 'made', 'get', 'far', 'etext', 'cannot', 'every', 'longest', 'during', 'him', 'areas', 'places', 'presented', 'did', 'turns', 'large', 'p', 'she', 'though', 'small', 'x', 'where', 'application', 'parted', 'smaller', 'says', 'ends', 'up', 'edition', 'second', 'are', 'further', 'nnnnnsary', 'best', 'even', 'what', 'said', 'ways', 'away', 'brief', 'yet', 'access', 'case', 'behind', 'smallest', 'above', 'between', 'new', 'ever', 'across', 'we', 'full', 'opening', 'men', 'vol', 'here', 'youngest', 'let', 'groups', 'others', 'alone', 'along', 'come', 'both', 'great', 'z', 'last', 'many', 'k', 'against', 's', 'became', 'faces', 'whole', 'asked', 'among', 'very', 'point', 'had', 'furthered', 'ask', 'ordered', 'everybody', 'furthers', 'use', 'or', 'from', 'working', 'yyyyyyyung', 'two', 'been', 'next', 'few', 'much', 'therefore', 'interest', 'h', 'taken', 'until', 'today', 'more', 'wants', 'asks', 'knows', 'becomes', 'c', 'downing', 'everywhere', 'known', 'cases', 'general', 'those', 'must', 'me', 'high', 'myself', 'room', 'being', 'f', 'these', 'work', 'as', 'n', 'will', 'while', 'anywhere', 'can', 'mr', 'making', 'my', 'guide', 'something', 'would', 'give', 'almost', 'is', 'j', 'thus', 'it', 'states', 'itself', 'numbers', 'want', 'in', 'anything', 'around', 'needs', 'end', 'thing', 'rather', 'things', 'make', 'same', 'orders', 'member', 'how', 'parts', 'fourth', 'plus', 'gets', 'showing', 'higher', 'used', 'see', 'again', 'may', 'after', 'upon', 'them', 'uses', 'never', 'such', 'yours', 'thoughts', 'man', 'a', 'off', 'backed', 'older', 'well', 'anybody', 'finds', 'q', 'perhaps', 'without', 'so', 'greatest', 'fundamental', 'keeps', 'y', 'the', 'essential', 'order', 'latest', 'newest', 'just', 'less', 'furthering', 'generally', 'over', 'years', 'ended', 'through', 'rooms', 'facts', 'still', 'its', 'before', 'group', 'grouped', 'somewhere', 'interesting', 'seems', 'better', 'lets', 'other', 'differently', 'has', 'might', 'gave', 'non', 'good', 'somebody', 'greater', 'big', 'downs', 'possible', 'early', 'know', 'they', 'not', 'now', 'd', 'several', 'like', 'always', 'l', 'someone', 't', 'each', 'become', 'went', 'works', 'side', 'grouping', 'either', 'everyone', 'old', 'often', 'series', 'some', 'back', 'year', 'our', 'thinks', 'out', 'backing', 'opened', 'for', 'although', 'since', 'per', 'everything', 'asking', 'got', 'three', 'shows', 'be', 'who', 'turning', 'seconds', 'given', 'quite', 'nowhere', 'members', 'put', 'approach', 'wanted', 'by', 'card', 'on', 'about', 'goods', 'puts', 'g', 'package', 'of', 'could', 'needing', 'o', 'keep', 'turn', 'place', 'w', 'enough', 'think', 'first', 'does', 'already', 'seeming', 'into', 'within', 'number', 'one', 'down', 'beings', 'because', 'done', 'long', 'another', 'open', 'your', 'differ', 'her', 'their', 'ordering', 'downed', 'there', 'least', 'sides', 'anyone', 'needed', 'too', 'way', 'hen', 'was', 'opens', 'gives', 'interests', 'himself', 'mostly', 'that', 'nobody', 'took', 'but', 'introduction', 'part', 'newer', 'fully', 'highest', 'herself', 'than', 'he', 'kind', 'b', 'third', 'showed', 'whether', 'largely', 'likely', 'us', 'different', 'r', 'were', 'problem', 'toward', 'clear', 'and', 'v', 'sees', 'certain', 'turned', 'together', 'an', 'say', 'right', 'at', 'have', 'need', 'seem', 'face', 'saw', 'clearly', 'thought', 'no', 'when', 'backs', 'also', 'interested', 'take', 'which', 'wanting', 'really', 'presenting', 'sure', 'howevhowevhoif', 'shall', 'any', 'knew', 'problems', 'wells', 'most', 'important', 'printed', 'nothing', 'why', 'parting', 'began', 'younger', 'e', 'longer', 'noone', 'later', 'm', 'worked', 'mrs', 'points', 'u', 'came', 'once', 'having', 'fact', 'evenly']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the data\n",
    "\"\"\"\n",
    "\n",
    "all_titles = [w.rstrip() for w in open('all_book_titles.txt','r')] # the for ..  in open(...) reads in lines...i.e. till the next new line\n",
    "                                                                   # rstrip removes the '\\n' from the end\n",
    "# TODO see what happens without rstrip\n",
    "\"\"\" TODO try this piece of code\n",
    "    with open('all_book_titles.txt','r') as f:\n",
    "            t = f.read()\n",
    "            print type(t),len(t)\n",
    "\"\"\"\n",
    "stop_words = set([w.rstrip() for w in open('stopwords.txt','r')]) # set datatype makes sure only unique entries\n",
    "                                                                  # are the in stop_words\n",
    "print stop_words\n",
    "# we want to add some stop words to this list, according the video. the set allows a union operation to happen\n",
    "stop_words = stop_words.union(['introduction', 'edition', 'series', 'application',\n",
    "    'approach', 'card', 'access', 'package', 'plus', 'etext',\n",
    "    'brief', 'vol', 'fundamental', 'guide', 'essential', 'printed',\n",
    "    'third', 'second', 'fourth'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wrong', 'this']\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\"\"\" Make the tokenization code \"\"\"\n",
    "def tokenizer(txt):\n",
    "    txt_lower = txt.lower() # make lower case\n",
    "    tokens = nltk.tokenize.word_tokenize(txt_lower) # make tokens TODO how is this different from split\n",
    "                                                    # try this: tokens = txt_lower.split()\n",
    "    \n",
    "    tokens = [w for w in tokens if len(w) > 2] # remove the short words\n",
    "    stemmed = [wordnet_lemmatizer.lemmatize(w) for w in tokens] # lemmatize, i.e. convert to the root word\n",
    "    pruned = [w for w in tokens if w not in stop_words] # remove the stop words\n",
    "    # This is extra for all_book_titles.txt , we remove stuff like 1st edition, 2nd edition\n",
    "    pruned2 = [w for w in pruned if not any(c.isdigit() for c in w)]\n",
    "    \"\"\" \n",
    "    this list comprehension is a bit complicated. It is a contaction of the code below.\n",
    "    The idea is skip a word if any of its elements is a digit\n",
    "    so check each element of the word:  for c in w \n",
    "    and if ANY of the c's are a digit: any(c.isdigit() for c in w)\n",
    "    then skip it: if not(any(c.isdigit() for c in w)) \n",
    "    look at this piece of code to get the logic\n",
    "    for w in pruned:\n",
    "        print w ,not any(c.isdigit() for c in w)\n",
    "    \"\"\"\n",
    "    \n",
    "    return pruned2\n",
    "    \n",
    "txt = 'Hi; what is wrong with this 4th?'\n",
    "print  tokenizer(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Tokenize each title, and make the vocabulary\n",
    "Not commented where it is similar to the sentiment analysis.\n",
    "\"\"\"\n",
    "word_index_map = {} # dictionary will map say 'hi' to 22\n",
    "index_word_map = [] # TODO  why is this a list while the one above is a dictionary?\n",
    "title_tokens = [] \n",
    "nwords = 0\n",
    "\n",
    "for t in all_titles:\n",
    "    \"\"\" There is a danger with ascii encoding in the book titles\n",
    "    so try-except whereever the encoding has a problem. Not sure why this is needed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        t = t.encode('ascii','ignore')\n",
    "        t_tokens = tokenizer(t)\n",
    "        title_tokens.append(t_tokens) \n",
    "        for w in t_tokens:\n",
    "            if w not in word_index_map:\n",
    "                word_index_map[w] = nwords\n",
    "                index_word_map.extend([w])\n",
    "                nwords += 1\n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2370"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "philosophy [ 1.  0.  0. ...,  0.  0.  0.] 31.0\n",
      "philosophy sex love reader\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert each Title to numeric form\n",
    "\"\"\"\n",
    "ntitles = len(title_tokens)\n",
    "X = np.zeros((nwords,ntitles))\n",
    "\n",
    "for i,t_tokens in enumerate(title_tokens): # enumerate(['a',99,'haha'])  will output tuples of the form\n",
    "                                           # (index,element) i.e. (0,'a'), (1,99) , (2,'haha')\n",
    "    for token in t_tokens:\n",
    "        X[word_index_map[token],i] = 1. # just an indicator variable\n",
    "\n",
    "# see what X is like\n",
    "print index_word_map[0],X[0,:],sum(X[0,:]) # first row, i.e. see which titles does the first word appear in? \n",
    "                                           # how many documents does it appear in\n",
    "for i in range(X.shape[0]): # first column, see which words appear in the first title\n",
    "    if X[i,0] == 1:\n",
    "        print index_word_map[i],\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do the SVD\n",
    "\"\"\"\n",
    "from sklearn import decomposition # on my computer if you dont do this, it says sklearn does not have decompostion\n",
    "svd = sklearn.decomposition.TruncatedSVD()\n",
    "Z = svd.fit_transform(X)\n",
    "print Z.shape # see the shape of the transformed words\n",
    "plt.plot(Z[:,0],Z[:,1]) # plot the first 2 dimensions of all the words\n",
    "for w in word_index_map:\n",
    "    i = wod_index_map[w]\n",
    "    plt.annotate(xy=(Z[i,0],Z[]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

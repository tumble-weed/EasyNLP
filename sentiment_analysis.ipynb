{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sentiment Analysis '"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Sentiment Analysis \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # for matrix operations - randomization, making arrays etc.\n",
    "from bs4 import BeautifulSoup # For parsing the reviews which are xml files\n",
    "import nltk # For all the NLP stuff\n",
    "import sklearn # For the learning part\n",
    "\n",
    "import pdb # python debugger for breakpoints etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Order of operations'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Order of operations\"\"\"\n",
    "# Get Data\n",
    "# Processing on the raw text : Tokenization, Lower-Case ,Lemmatization, Stop words etc. etc.\n",
    "# build the vocabulary (as a word_index_map)\n",
    "# Make the numeric data\n",
    "# learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "[<review_text>\\nI purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power.\\n\\nI feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply.\\n\\nAs always, Amazon had it to me in &lt;2 business days\\n</review_text>, <review_text>\\nI ordered 3 APC Back-UPS ES 500s on the recommendation of an employee of mine who used to work at APC. I've had them for about a month now without any problems. They've functioned properly through a few unexpected power interruptions. I'll gladly order more if the need arises.\\n\\nPros:\\n - Large plug spacing, good for power adapters\\n - Simple design\\n - Long cord\\n\\nCons:\\n - No line conditioning (usually an expensive option\\n</review_text>]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "the review files are XML, we are interested in the tag review_text\n",
    "\"\"\"\n",
    "review_files = ['electronics/positive.review','electronics/negative.review']\n",
    "pos_rev = BeautifulSoup(open(review_files[0],'r').read())\n",
    "pos_rev = pos_rev.findAll('review_text')\n",
    "\n",
    "print type(pos_rev) # While pos_rev is a class object, it is actually a wrapper around a list.\n",
    "print pos_rev[:2] # each element of the list is a review_text (with the tag intact)\n",
    "\n",
    "# similar for the negative review\n",
    "neg_rev = BeautifulSoup(open(review_files[1],'r').read())\n",
    "neg_rev = neg_rev.findAll('review_text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_rev),len(pos_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer # Uses WordNet to find the words to stem to\n",
    "import pdb\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = [w.rstrip() for w in open('stopwords.txt','r')] # the for ..  in open(...) reads in lines...i.e. till the next new line\n",
    "                                                             # rstrip removes the '\\n' from the end\n",
    "def process_text(txt):\n",
    "    txt_lower = txt.lower() # convert all the text to lower case\n",
    "    tokens = nltk.tokenize.word_tokenize(txt_lower) # Get all the words\n",
    "    tokens = [t for t in tokens if len(t)> 2]# remove all the 2 letter tokens \"in,on\" etc.\n",
    "    stemmed = [lemmatizer.lemmatize(t) for t in tokens] # stem all the words\n",
    "    stemmed = [w for w in stemmed if w not in stop_words] # remove all the stop words\n",
    "    return stemmed\n",
    "    pass\n",
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'money': 113, u'valuable': 28, u'month': 44, u'functioned': 45, u'hole': 94, u'carpet': 68, u'slot': 80, u'monitor': 14, u'fit': 92, u'simple': 56, u'easy': 67, u'employee': 41, u'save': 17, u'...': 96, u'easily': 105, u'break': 89, u'lcd': 13, u'putting': 85, u'day': 36, u'minute': 15, u'wa': 86, u'loss': 27, u'receiving': 21, u'cable': 10, u\"'ve\": 43, u'bad': 8, u'dispite': 109, u'rack': 77, u'blackout': 5, u'interruption': 48, u'unexpected': 47, u'individual': 79, u'design': 57, u'recommendation': 40, u'investment': 24, u'business': 35, u'bottom': 81, u'expensive': 63, u'electronics': 20, u'poorly': 74, u'bought': 88, u'cord': 58, u'shut': 18, u'run': 9, u'vertical': 76, u'loose': 99, u'spacing': 54, u'rug': 107, u'conditioning': 61, u'plug': 53, u'irregular': 33, u'adapter': 55, u'usually': 62, u'router': 12, u'frequent': 4, u'con': 59, u'equally': 19, u'supply': 7, u'feel': 23, u'ca': 98, u'cd': 71, u'doesnt': 78, u'apc': 37, u'unit': 2, u'guess': 101, u'data': 29, u'top': 73, u'due': 3, u'lot': 70, u'spike': 32, u'minor': 25, u'box': 111, u'arises': 51, u'option': 64, u'gap': 97, u'failure': 30, u'properly': 46, u'basically': 83, u'gladly': 50, u'line': 60, u'hold': 102, u'pull': 84, u'stacked': 72, u'look': 104, u'this': 1, u'500': 39, u'piece': 90, u'guide': 93, u'nicer': 115, u'steady': 106, u'pro': 52, u'again..poorly': 95, u'mine': 42, u'surface': 108, u'purchased': 0, u'compared': 26, u\"'ll\": 49, u'tip': 65, u'power': 6, u'equipment': 31, u'fitting': 100, u'you': 69, u'nice': 103, u'picture': 110, u'pain': 87, u'advice': 112, u'designed': 75, u'extremely': 66, u'stack': 82, u'back-ups': 38, u'sturdy': 116, u'invest': 114, u'metal': 91, u'amazon': 34, u'clean': 22, u'time': 16, u'modem': 11}\n",
      "117\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dry Run 1, see how the loops work\n",
    "\"\"\"\n",
    "pos_tokens = [] # empty lists for storing the tokens derived from the reviews\n",
    "neg_tokens = [] # empty lists for storing the tokens derived from the reviews\n",
    "word_index_map = {} # a dictionary that maps a word to an index\n",
    "nwords = 0\n",
    "nrev = 0 # number of reviews\n",
    "\n",
    "for rev,rev_tokens in zip([pos_rev,neg_rev],[pos_tokens,neg_tokens]):  \n",
    "    # zip lets us iterate over multiple lists simultaneously\n",
    "    nrev = 0 \n",
    "    for txt in rev:\n",
    "        this_tokens = process_text(txt.text) # tokens from this review\n",
    "        rev_tokens.append(this_tokens)# add it to the list for this sentiment\n",
    "        for t in this_tokens:\n",
    "            if t not in word_index_map: # if the word is not already seen\n",
    "                word_index_map[t] = nwords # add it to the word_index_map\n",
    "                nwords +=1 \n",
    "        nrev += 1\n",
    "        if nrev >1: # just a break for the dry run\n",
    "            break\n",
    "print word_index_map # see what the word index map looks like after have seen 2 positive and 2 negative reviews\n",
    "print nwords\n",
    "print len(pos_tokens),nrev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Actual Loop to make tokens out of the reviews\n",
    "\"\"\"\n",
    "pos_tokens = []\n",
    "neg_tokens = []\n",
    "word_index_map = {}\n",
    "nwords = 0\n",
    "nrev = 0\n",
    "for rev,rev_tokens in zip([pos_rev,neg_rev],[pos_tokens,neg_tokens]): \n",
    "    for txt in rev:\n",
    "        this_tokens = process_text(txt.text)\n",
    "        rev_tokens.append(this_tokens)\n",
    "        for t in this_tokens:\n",
    "            if t not in word_index_map:\n",
    "                word_index_map[t] = nwords\n",
    "                nwords +=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert all the tokenized reviews to numeric data\n",
    "\"\"\"\n",
    "pos_numeric = [] # numeric form for the positive reviews\n",
    "neg_numeric = []\n",
    "for sentiment_numeric,sentiment_tokens,class_label in zip([pos_numeric,neg_numeric],[pos_tokens,neg_tokens],[1,0]):\n",
    "    # zip over the numerics, tokens, and labels for each entiment\n",
    "    for rev in sentiment_tokens: # each review in the tokens for this sentiment\n",
    "        this_numeric = np.zeros((nwords+1,)) # make a zeros of size (vocabulary size + (1 for the label) )\n",
    "        for w in rev:\n",
    "            this_numeric[word_index_map[w]] += 1 # each word in review adds to its index\n",
    "        this_numeric = this_numeric/sum(this_numeric) # so that it sums to 1, \n",
    "                                                      # makes the feature invariant to number of words in the review\n",
    "        this_numeric[-1] = class_label # last element is the sentiment label\n",
    "        sentiment_numeric.append(this_numeric) # append this review to the sentiments numeric list\n",
    "    \n",
    "# we'll want to use numpy functions now, so convert the numeric lists to numpy arrays\n",
    "pos_numeric = np.array(pos_numeric) \n",
    "neg_numeric = np.array(neg_numeric)\n",
    "np.random.shuffle(pos_numeric) # IN PLACE shuffle\n",
    "np.random.shuffle(neg_numeric) # IN PLACE shuffle\n",
    "\n",
    "n_neg_rev = len(neg_rev)\n",
    "n_pos_rev = len(pos_rev)\n",
    "samples_per_sentiment = min(n_neg_rev,n_pos_rev) # in case the 2 sentiments had different number of reviews\n",
    "\n",
    "pos_numeric = pos_numeric[:samples_per_sentiment+1,:] # keep the minimum number of reviews\n",
    "neg_numeric = neg_numeric[:samples_per_sentiment+1,:]\n",
    "\n",
    "xy = np.concatenate((pos_numeric,neg_numeric)) # make the combined dataset\n",
    "np.random.shuffle(xy) # shuffle so that pos and neg are randomly distibuted across the dataset\n",
    "X = xy[:,:-1] # break down into feature X\n",
    "Y = xy[:,-1] # and labels Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make the training and test sets\n",
    "# test is the last 100 rows, and the train is the rest\n",
    "Xtr = X[:-100,:]\n",
    "Ytr = Y[:-100]\n",
    "\n",
    "Xts = X[-100:,:]\n",
    "Yts = Y[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.780000 , Test Accuracy 0.680000\n"
     ]
    }
   ],
   "source": [
    "# Make a logistic regression classifier\n",
    "log_reg = sklearn.linear_model.LogisticRegression()\n",
    "log_reg.fit(Xtr,Ytr) # fit to the train\n",
    "print 'Train Accuracy %f , Test Accuracy %f'%(log_reg.score(Xtr,Ytr),log_reg.score(Xts,Yts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11091)\n"
     ]
    }
   ],
   "source": [
    "# Explore the weights\n",
    "print log_reg.coef_.shape # coef_ stores the coefficients/weights \n",
    "word_weights = log_reg.coef_[0,:]\n",
    "index_word_map = dict(zip(word_index_map.values(),word_index_map.keys())) \n",
    "# this is the reverse of the vocabulary dictionary, maps from index to the word \n",
    "# this syntax is standard, keys and values the original dictionary's keys and values\n",
    "\n",
    "thresh = 0.5 # same as from lecture\n",
    "sig_idx=[] # indices of significant words\n",
    "sig_weights = [] # the logistic regression weights for these words\n",
    "\n",
    "# loop to keep the significant words and indices\n",
    "for idx in range(nwords): \n",
    "    if word_weights[idx] > thresh or word_weights[idx] < -thresh:\n",
    "#         print index_word_map[idx], word_weights[idx]\n",
    "        sig_idx.extend([idx])\n",
    "        sig_weights.extend([word_weights[idx]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Significant Positive\n",
      "\n",
      "price 2.84765236511\n",
      "easy 1.82483671107\n",
      "quality 1.58858499785\n",
      "excellent 1.41636015886\n",
      "love 1.21764470522\n",
      "you 1.1744308924\n",
      "sound 1.10778914646\n",
      "little 1.00895974045\n",
      "memory 0.998228889794\n",
      "perfect 0.970594408271\n",
      "fast 0.929110568025\n",
      "highly 0.914956423304\n",
      "speaker 0.851184743616\n",
      "'ve 0.822992039887\n",
      "lot 0.764332900057\n",
      "pretty 0.755552463275\n",
      "happy 0.69205100046\n",
      "cable 0.689636106396\n",
      "recommend 0.679645962543\n",
      "bit 0.674672116113\n",
      "using 0.669429206461\n",
      "comfortable 0.625733109924\n",
      "space 0.618130611566\n",
      "picture 0.588406375074\n",
      "ha 0.581043209685\n",
      "paper 0.572210536955\n",
      "expected 0.547978801697\n",
      "value 0.540389468463\n",
      "video 0.528032517014\n",
      "home 0.512271512669\n",
      "\n",
      "\n",
      "Most Significant Negative\n",
      "\n",
      "n't -2.02656043081\n",
      "wa -1.6730906974\n",
      "doe -1.26445720469\n",
      "return -1.17250643504\n",
      "then -1.05140342941\n",
      "money -1.02772474092\n",
      "item -0.980317302503\n",
      "waste -0.950470617889\n",
      "support -0.905622417865\n",
      "buy -0.804823906648\n",
      "tried -0.778415033022\n",
      "returned -0.762839766864\n",
      "poor -0.756632228452\n",
      "month -0.728957372861\n",
      "week -0.722580969327\n",
      "bad -0.710864415573\n",
      "unit -0.699511982055\n",
      "customer -0.673320791576\n",
      "time -0.65133859465\n",
      "try -0.646621837988\n",
      "warranty -0.622449904545\n",
      "refund -0.605274888653\n",
      "company -0.546146636545\n",
      "hour -0.543907171863\n",
      "stopped -0.535464893309\n",
      "junk -0.525911220745\n",
      "... -0.519803339192\n",
      "terrible -0.516078817407\n",
      "static -0.500754908052\n"
     ]
    }
   ],
   "source": [
    "# we want to see the words from most significant to least \n",
    "# so sort the ABSOLUTE VALUE of weights in descending, and then order the indices according to the weights' sort order\n",
    "\n",
    "sig_order = np.argsort(np.abs(sig_weights))[::-1] # sort defaults to ascending order, we want descending\n",
    "sorted_sig_weights = np.array(sig_weights)[sig_order] # weights sorted in highest to lowest\n",
    "sorted_sig_idx = np.array(sig_idx)[sig_order] # indices sorted according to the weight sorting\n",
    "\n",
    "print 'Most Significant Positive\\n'\n",
    "for idx,wt in zip(sorted_sig_idx,sorted_sig_weights):\n",
    "    if wt > thresh:\n",
    "        print index_word_map[idx],wt\n",
    "\n",
    "print '\\n\\nMost Significant Negative\\n'\n",
    "for idx,wt in zip(sorted_sig_idx,sorted_sig_weights):\n",
    "    if wt < -thresh:\n",
    "        print index_word_map[idx],wt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
